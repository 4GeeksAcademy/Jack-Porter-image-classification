{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification: dogs & cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2\n"
     ]
    }
   ],
   "source": [
    "# Handle imports up-front\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Silence logging messages from TensorFlow, except errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# PyPI imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import image from keras correctly based on the TensorFlow version\n",
    "tf_version = float('.'.join(tf.__version__.split('.')[0:2]))\n",
    "print(f'Tensorflow version {tf_version}')\n",
    "\n",
    "if tf_version > 2.8:\n",
    "    import keras.utils as image\n",
    "\n",
    "else:\n",
    "    from keras.preprocessing import image\n",
    "\n",
    "# Figure out if we are running on Kaggle or not, if so\n",
    "# add the location of utils.py to path so we can import\n",
    "path_list = os.getcwd().split(os.sep)\n",
    "\n",
    "if path_list[1] == 'kaggle':\n",
    "    sys.path.append('/kaggle/usr/lib/image_classification_functions')\n",
    "\n",
    "# Import custom helper functions from utils.py\n",
    "from image_classification_functions import prep_data\n",
    "from image_classification_functions import single_training_run\n",
    "from image_classification_functions import plot_single_training_run\n",
    "from image_classification_functions import hyperparameter_optimization_run\n",
    "from image_classification_functions import plot_hyperparameter_optimization_run\n",
    "\n",
    "# Silence logging messages from TensorFlow, except errors\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Kaggle notebook\n",
      "Checking data prep\n",
      "Missing images, final count: 0\n",
      "Running data prep\n",
      "Image archive should be at ../data/images/raw/dogs-vs-cats.zip\n",
      "Extracting ../data/images/raw/dogs-vs-cats.zip\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'train.zip' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Decompress and organize the images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_data_path, validation_data_path, testing_data_path = \u001b[43mprep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get lists of training and validation dog and cat images\u001b[39;00m\n\u001b[32m      5\u001b[39m training_dogs = glob.glob(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/dogs/dog.*\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Jack-Porter-image-classification/notebooks/image_classification_functions.py:577\u001b[39m, in \u001b[36mprep_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    574\u001b[39m     environment=\u001b[33m'\u001b[39m\u001b[33mother\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m environment == \u001b[33m'\u001b[39m\u001b[33mother\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m     training_data_path, validation_data_path, testing_data_path=\u001b[43mother_env_data_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m environment == \u001b[33m'\u001b[39m\u001b[33mkaggle\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    580\u001b[39m     training_data_path, validation_data_path, testing_data_path=kaggle_env_data_prep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Jack-Porter-image-classification/notebooks/image_classification_functions.py:618\u001b[39m, in \u001b[36mother_env_data_prep\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_filepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile.ZipFile(archive_filepath, mode=\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archive:\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m         \u001b[43marchive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain.zip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mraw_image_directory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdogs-vs-cats.zip already extracted\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/zipfile.py:1664\u001b[39m, in \u001b[36mZipFile.extract\u001b[39m\u001b[34m(self, member, path, pwd)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1662\u001b[39m     path = os.fspath(path)\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/zipfile.py:1703\u001b[39m, in \u001b[36mZipFile._extract_member\u001b[39m\u001b[34m(self, member, targetpath, pwd)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract the ZipInfo object 'member' to a physical\u001b[39;00m\n\u001b[32m   1700\u001b[39m \u001b[33;03m   file on the path targetpath.\u001b[39;00m\n\u001b[32m   1701\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(member, ZipInfo):\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m     member = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;66;03m# build the destination pathname, replacing\u001b[39;00m\n\u001b[32m   1706\u001b[39m \u001b[38;5;66;03m# forward slashes to platform specific separators.\u001b[39;00m\n\u001b[32m   1707\u001b[39m arcname = member.filename.replace(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m, os.path.sep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/zipfile.py:1476\u001b[39m, in \u001b[36mZipFile.getinfo\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1474\u001b[39m info = \u001b[38;5;28mself\u001b[39m.NameToInfo.get(name)\n\u001b[32m   1475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   1477\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m in the archive\u001b[39m\u001b[33m'\u001b[39m % name)\n\u001b[32m   1479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[31mKeyError\u001b[39m: \"There is no item named 'train.zip' in the archive\""
     ]
    }
   ],
   "source": [
    "# Decompress and organize the images\n",
    "training_data_path, validation_data_path, testing_data_path = prep_data()\n",
    "\n",
    "# Get lists of training and validation dog and cat images\n",
    "training_dogs = glob.glob(f'{training_data_path}/dogs/dog.*')\n",
    "training_cats = glob.glob(f'{training_data_path}/cats/cat.*')\n",
    "validation_dogs = glob.glob(f'{validation_data_path}/dogs/dog.*')\n",
    "validation_cats = glob.glob(f'{validation_data_path}/cats/cat.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training path:\", training_data_path)\n",
    "print(\"Validation path:\", validation_data_path)\n",
    "print(\"Testing path:\", testing_data_path)\n",
    "\n",
    "print(\"Number of training cat images:\", len(training_cats))\n",
    "print(\"Number of training dog images:\", len(training_dogs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the cat and dog images\n",
    "fig, axs = plt.subplots(3, 2, figsize=(6, 4))\n",
    "\n",
    "print(\"Number of cat images:\", len(training_cats))\n",
    "print(\"Number of dog images:\", len(training_dogs))\n",
    "\n",
    "for cat, dog, row in zip(training_cats, training_dogs, axs):\n",
    "    print(\"Cat path:\", cat)\n",
    "    print(\"Dog path:\", dog)\n",
    "\n",
    "    for animal, ax in zip([cat, dog], row):\n",
    "        animal = image.load_img(animal)\n",
    "        animal = image.img_to_array(animal)\n",
    "        animal /= 255.0\n",
    "        ax.imshow(animal)\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "Let's take a look at a few of our images to get a feel for how image data is structured.\n",
    "\n",
    "### 2.1. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the images as an array and look at it's shape - what do you see, what are the dimensions? Are they what you expect?\n",
    "\n",
    "# Load a sample image \n",
    "sample_image_path = training_dogs[0]\n",
    "\n",
    "# Load the image\n",
    "sample_image = image.load_img(sample_image_path)\n",
    "\n",
    "# Convert to array\n",
    "sample_array = image.img_to_array(sample_image)\n",
    "\n",
    "# Print shape\n",
    "print(f\"Image shape: {sample_array.shape}\") \n",
    "\n",
    "# Display the image\n",
    "plt.imshow(sample_array.astype(\"uint8\"))\n",
    "plt.title(\"Sample Dog Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the three 2D arrays which comprise the image. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Image dimensions\n",
    "\n",
    "Let's take a look at a random sample of images from the dataset and see what their dimensions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over a few hundred images and extract their width and heigh, plot both as a histogram. What do you see, does this information matter to us, if so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Image aspect ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the image aspect ratios (i.e. width/height) What do you see, does this information matter to us, if so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model\n",
    "\n",
    "### 3.1. Prepare images for streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(\n",
    "        training_data_path: str,\n",
    "        image_width: int,\n",
    "        image_height: int, \n",
    "        batch_size: int=32,\n",
    "        steps_per_epoch: int=50,\n",
    "        epochs: int=10\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "\n",
    "    training_dataset, validation_dataset=tf.keras.utils.image_dataset_from_directory(\n",
    "        training_data_path,\n",
    "        validation_split=0.2,\n",
    "        subset='both',\n",
    "        seed=315,\n",
    "        shuffle=True,\n",
    "        image_size=(image_width, image_height),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    epoch_images=batch_size*steps_per_epoch\n",
    "    total_images=epoch_images*epochs\n",
    "\n",
    "    training_dataset=training_dataset.cache().shuffle(total_images, reshuffle_each_iteration=True).prefetch(buffer_size=total_images).repeat()\n",
    "    validation_dataset=training_dataset.cache().shuffle(total_images, reshuffle_each_iteration=True).prefetch(buffer_size=total_images).repeat()\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "\n",
    "\n",
    "training_dataset, validation_dataset=make_datasets(\n",
    "    training_data_path,\n",
    "    image_width=64,\n",
    "    image_height=48,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=50,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(image_width, image_height, learning_rate):\n",
    "\n",
    "    initializer=tf.keras.initializers.GlorotUniform(seed=315)\n",
    "\n",
    "    model=Sequential([\n",
    "        layers.Input((image_width, image_height, 3)),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu', kernel_initializer=initializer),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu', kernel_initializer=initializer),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu', kernel_initializer=initializer),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "        layers.Dense(1, activation='sigmoid', kernel_initializer=initializer)\n",
    "    ])\n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model=compile_model(\n",
    "    image_width=64,\n",
    "    image_height=48, \n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_results=model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=100,\n",
    "  steps_per_epoch=50,\n",
    "  validation_steps=50,\n",
    "  verbose=0\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look the information 'training_results' contains. Plot the training \n",
    "# and validation accuracy (and binary cross-entropy if you like) over the\n",
    "# training epochs. Is the model learning? If not, what do you think\n",
    "# is wrong?\n",
    " \n",
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('CNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].plot(np.array(training_results.history['binary_accuracy']) * 100, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_binary_accuracy']) * 100, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "# Plot training and validation binary cross-entropy\n",
    "axs[1].set_title('Binary cross-entropy')\n",
    "axs[1].plot(training_results.history['loss'])\n",
    "axs[1].plot(training_results.history['val_loss'])\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Binary cross-entropy')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try optimizing the learning rate and the batch size using a few values near the default settings. Hint: use a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best values for learning rate and batch size and train the model for longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model out on the test data - is it as good as you expected, given the training data? Worse? Better? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
